---
title: "HW4StatsLearn"
---

# HW 4 Stats Learn

```{r}
library(ISLR)
library(e1071) #For Svm
```

## Question 1:

a.) A real life data example of when a false negative would be less tolerable than a false positive would be a medical screening for a leathal infectious disease.

b.) A real life data example of when a false positive would be less tolerable than a false negative would be in the case of a security system, should someone break in or not, to know would be more favorable than to not know and someone actually break in to your home.

c.) In the case where a false positive and a false negative are of equal importance would be in the case of a cancer screening, having the illness and thinking you have the illness would still result in the same amount of anxiety and harmful chemo treatment should you choose to seek that.

## Question 2:

a.)

```{r}
attach(USArrests)
```

```{r}
pr.out <- prcomp(USArrests,scale=TRUE)
(pr.out$sdev)
```

```{r}
pr.var <- pr.out$sdev^2
pve <- pr.var / sum(pr.var)
(pve)
```

```{r}
#present our results in plot
plot(pve, xlab="Principal Component", ylab=" Proportion of Variance Explained ",ylim=c(0,1) ,type='b')
```

An explanation: As you can see, the level of variance tends to decrease as the Principal Component iterates up (PCA are elements of the Natural number system only). This is because each Principle components impact decreases from that of the first, thus making the first the most important and the last the last significant.

b.)

```{r}
#obtain loadings from prcomp() function
loadings<-pr.out$rotation
#scale dataset just to make sure data we us is consistent
USArrests2 <- scale(USArrests)
#convert dataset into matrix, square each value in matrix, and sum them up 
#to get the denominator of the equation
sumvalue<-sum(as.matrix(USArrests2)^2)
#multiple these two matrix and then sqaure
num<-(as.matrix(USArrests2)%*%loadings)^2
#calculate the column value for num matrix
colvalue<-c()
for (i in 1:length(num[1,])){
  colvalue[i]<-sum(num[,i])
}
#calculate new pve
pve1<-colvalue/sumvalue
(pve1)
```

## Question 3:

```{r}
set.seed(1)
x1 <- rnorm(200)
x2 <- 4 * x1^2 + 1 + rnorm(200)
y <- as.factor(c(rep(1,100), rep(-1,100)))
x2[y==1] <- x2[y==1] + 3
x2[y==-1] <- x2[y==-1]-3
plot (x1[y==1], x2[y==1], col = "red", xlab = "X", ylab = "Y", ylim = c(-6, 30))
points(x1[y==-1], x2[y==-1], col = "blue")
myDat <- data.frame(x1,x2,y)
```

```{r}
set.seed(1)
train_index <- sample(1:nrow(myDat), size = 0.8 * nrow(myDat), replace = F)
train_data <- myDat[train_index, ]  # 80% training data
test_data  <- myDat[-train_index, ] # 20% testing data

# Check dimensions
#dim(train_data)
#dim(test_data)
```

i.)

```{r}
set.seed(1)
svm_linear <- svm(y ~ . , kernel = "linear", data = train_data, cost = 0.01)
summary(svm_linear)

```

Error Rate Function:

```{r}
# calculate error rate
calc_error_rate <- function(svm_model, dataset, true_classes) {
  confusion_matrix <- table(predict(svm_model, dataset), true_classes)
  return(1 - sum(diag(confusion_matrix)) / sum(confusion_matrix))
}

```

So things are clear: What is happening here is a function declaration (calc_error_rate) we will be using it a few times so it is best to have it ready to go every time. So we pass a svm model, a data set (train or testing) and the true classes (response variable data). we then pass the svm model and the data set into the built in prediction function. and we take that out put as well as the true classes and put those two values into a table and store it into a variable appropriately called "confusion matrix". We then return from the function 1 - the sum of the correct entries of the confusion matrix divided by all the entries in the confusion matrix. It is a bit esoteric but once you understand confusion matrix and how to calculate error it is very straightforward.

Training error:

```{r}
cat("Training Error Rate:", 100 * calc_error_rate(svm_linear, train_data, train_data$y), "%\n")

```

Testing error:

```{r}
cat("Test Error Rate:", 100 * calc_error_rate(svm_linear, test_data, test_data$y), "%\n")
```

Tune for linear Svm model with respect to cost.

```{r}
set.seed(1)
svm_tune_linear <- tune(svm, y ~ . , data = myDat, kernel = "linear", 
                  ranges = list(cost = seq(0.01, 10, length = 25)))
summary(svm_tune_linear)

```

Tuned training error:

```{r}
set.seed(1)
svm_linear <- svm(y ~ . , kernel = "linear", 
                  data = train_data, cost = svm_tune_linear$best.parameters$cost)

cat("Training Error Rate:", 100 * calc_error_rate(svm_linear, train_data, train_data$y), "%\n")
```

Tuned testing error:

```{r}
cat("Test Error Rate:", 100 * calc_error_rate(svm_linear, test_data, test_data$y), "%\n")
```

ii\)

```{r}
set.seed(1)
svm_poly <- svm(y ~ . , data = train_data, kernel = "poly", degree = 2, cost = .01)
summary(svm_poly)
```

Training error:

```{r}
cat("Training Error Rate:", 100 * calc_error_rate(svm_poly, train_data, train_data$y), "%\n")

```

Testing error:

```{r}
cat("Training Error Rate:", 100 * calc_error_rate(svm_poly, test_data, test_data$y), "%\n")
```

Tune

```{r}
set.seed(1)
svm_tune_poly <- tune(svm, y ~ . , data = myDat, kernel = "poly",
                  ranges = list(cost = seq(0.01, 10, length = 25)))
summary(svm_tune_poly)
```

Tuned training error:

```{r}
set.seed(1)
svm_poly <- svm(y ~ . , data = myDat, kernel = "poly",
                   cost = svm_tune_poly$best.parameters$cost)

cat("Training Error Rate:", 100 * calc_error_rate(svm_poly, train_data, train_data$y), "%\n")
```

Tuned testing error:

```{r}
cat("Training Error Rate:", 100 * calc_error_rate(svm_poly, test_data, test_data$y), "%\n")
```

iii.)

```{r}
set.seed(1)
svm_radial <- svm(y ~ . , data = train_data, kernel = "radial")
summary(svm_radial)
```

Training error:

```{r}
cat("Training Error Rate:", 100 * calc_error_rate(svm_radial, train_data, train_data$y), "%\n")
```

Testing error:

```{r}
cat("Test Error Rate:", 100 * calc_error_rate(svm_radial, test_data, test_data$y), "%\n")
```

Tune

```{r}
set.seed(1)
svm_tune_radial <- tune(svm, y ~ . , data = train_data, kernel = "radial",
                  ranges = list(cost = seq(0.01, 10, length = 25)))
summary(svm_tune_radial)
```

Tuned training error:

```{r}
svm_radial <- svm(y ~ . , data = train_data, kernel = "radial",
                   cost = svm_tune_radial$best.parameters$cost)

cat("Training Error Rate:", 100 * calc_error_rate(svm_radial, train_data, train_data$y), "%\n")
```

Tuned testing error:

```{r}
cat("Test Error Rate:", 100 * calc_error_rate(svm_radial, test_data, test_data$y), "%\n")
```

## Question 4

a.)

```{r}
median <- median(Auto$mpg)
Auto$high <- ifelse(Auto$mpg > median, 1, 0)
```

b.)

```{r}
set.seed(1)
auto_tune <- tune(svm,high~.,data=Auto,kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(auto_tune)
```

Tuned train error

```{r}

```

Tuned test error

```{r}

```

c.)

Tuned Radial Svm

```{r}
set.seed(1)
auto_tune_radial <- tune(svm,high~.,data=Auto, kernel="radial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(auto_tune_radial)
```

Tuned Polynomial Svm

```{r}
set.seed(1)
auto_tune_poly <- tune(svm,high~.,data=Auto,kernel="polynomial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4),degree=c(2,3,4)))
summary(auto_tune_poly)
```
